{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_okuN9-LDD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47c5df8-956e-44f7-f177-78cca5fab3f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.7/618.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.4/278.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.2/279.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/274.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.9/273.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.0/273.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip  install openai langchain langchain-community faiss-cpu langchain-openai tiktoken >/dev/null\n",
        "!pip install -q langchain_openai==0.0.2 faiss-cpu==1.7.4 openai==1.6.1 tiktoken==0.5.2 langchain_community==0.0.11 langchain==0.1.0 python-telegram-bot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "from google.colab import output\n",
        "from google.colab import userdata\n",
        "\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler\n",
        "from telegram import InlineKeyboardMarkup, Update, InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup\n",
        "from telegram import Update\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import requests\n",
        "import aiohttp\n",
        "import json\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
        "#from langchain_openai import OpenAI\n",
        "from openai import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "import re\n",
        "import requests\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "hZCr7JsMNzT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Используем секретный ключ в колабе\n",
        "from google.colab import userdata\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = key"
      ],
      "metadata": {
        "id": "FuzI9KmpQ-QS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title функция для загрузки документа по ссылке из гугл драйв\n",
        "def load_document_text(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "kEhnPmib18q1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title База знаний, которая будет подаваться в langChain\n",
        "#data_from_url= load_document_text('https://docs.google.com/document/d/1ova0gprJGQ7_wYJhxEozpzZTgsGYYgyADeNOm-l7rfM')\n",
        "data_from_url = load_document_text('https://docs.google.com/document/d/1Tr7DfGNP9V1PcwTOhywHImT12h5krrmWapTTvyXyaDI') #Calendar\n",
        "system_from_url = load_document_text('https://docs.google.com/document/d/1IESDe-22St33o8AKCIgDX2RLuSzILcOq8dxVUPWsjbU') #Jane"
      ],
      "metadata": {
        "id": "cwIlS42M19xb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Разбиениe бд recursive cts\n",
        "\n",
        "# source_chunks=[]\n",
        "# splitter = RecursiveCharacterTextSplitter(chunk_size=2024, chunk_overlap=0)\n",
        "# for chunk in splitter.split_text(data_from_url):\n",
        "#     source_chunks.append(Document(page_content=chunk, metadata={\"meta\":\"data\"}))\n",
        "# len(source_chunks)"
      ],
      "metadata": {
        "id": "OyRQvFpW2PU_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Разбиениe бд MarkDown\n",
        "source_chunks=[]\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "]\n",
        "\n",
        "# MD splits\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
        ")\n",
        "source_chunks = markdown_splitter.split_text(data_from_url)\n",
        "len(source_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b001da-cc43-4b32-f4ca-dc3e0b1284c2",
        "id": "TT8EF5YcDf1I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализирум модель эмбеддингов\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Создадим индексную базу из разделенных фрагментов текста\n",
        "db = FAISS.from_documents(source_chunks, embeddings)"
      ],
      "metadata": {
        "id": "oTeIFnJX2B4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51281f95-8ddf-43ed-cf15-59b31872656b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAIEmbeddings instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title определяем переменные и промпты\n",
        "system_prompt = system_from_url\n",
        "#model = \"gpt-3.5-turbo-0125\"\n",
        "#model = \"gpt-4-0125-preview\"\n",
        "model = \"gpt-4-turbo-2024-04-09\"\n",
        "option = True\n",
        "temperature = 0.4\n",
        "num_fragment = 4\n",
        "verbose = 0\n",
        "user_prompt = \"Information to answer user question, consider dialogue history and supplied information from the schedule.\"\n",
        "user_prompt_2 = \"You need to collect the information about desired meeting option from the user to setup the meeting with Mr Erik, consider dialogue history and supplied information from the schedule.\"\n",
        "summary_memory = ConversationSummaryMemory(llm=ChatOpenAI(model_name=model))"
      ],
      "metadata": {
        "id": "NrhqueOXzZh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sumarize old version\n",
        "# def summarize_questions(dialog):\n",
        "#     \"\"\"\n",
        "#     Функция возвращает саммаризированный текст диалога.\n",
        "#     \"\"\"\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": \"You are an AI summarizer, your task is to make a summary of a dialogue. Collect all relevant and important information from the text. If user mentions his name, add it to the summary\"},\n",
        "#         {\"role\": \"user\", \"content\": \"Make a summary of this dialogue between users: \" + \" \".join(dialog)}\n",
        "#     ]\n",
        "\n",
        "#     completion = openai.ChatCompletion.create(\n",
        "#        # model=\"gpt-4-0613\",     # используем gpt4 для более точной саммаризации\n",
        "#         model = \"gpt-3.5-turbo-16k\",\n",
        "#         messages=messages,\n",
        "#         temperature=0,          # Используем более низкую температуру для более определенной суммаризации\n",
        "#     )\n",
        "#     print('summary: '+completion.choices[0].message.content)\n",
        "#     return completion.choices[0].message.content\n",
        "\n",
        "# def answer_user_question_dialog(system, db, user_question, question_history):\n",
        "#     \"\"\"\n",
        "#     Функция возвращает ответ на вопрос пользователя.\n",
        "#     \"\"\"\n",
        "#     summarized_history = \"\"\n",
        "#     # Если в истории более одного вопроса, применяем суммаризацию\n",
        "#     if len(question_history) > 0:\n",
        "#         summarized_history = \"Previous dialogue: \" + summarize_questions([q + ' ' + (a if a else '') for q, a in question_history])\n",
        "#         #dialogue = ' '.join([q + ' ' + (a if a else '') for q, a in question_history])\n",
        "#         #summarized_history = \"Previous dialogue: \" + summarize_questions(dialogue)\n",
        "\n",
        "#     # Добавляем явное разделение между историей диалога и текущим вопросом\n",
        "#     input_text = summarized_history + \"\\n\\nCurrent user question: \" + user_question\n",
        "\n",
        "#     # Извлекаем наиболее похожие отрезки текста из базы знаний и получение ответа модели\n",
        "#     answer_text = answer_index(system, input_text, db)\n",
        "\n",
        "#     # Добавляем вопрос пользователя и ответ системы в историю\n",
        "#     question_history.append((user_question, answer_text if answer_text else ''))\n",
        "\n",
        "#     # Выводим саммаризированный текст, который видит модель\n",
        "#     if summarized_history:\n",
        "#         print('****************************')\n",
        "#         print(insert_newlines(summarized_history))\n",
        "#         print('****************************')\n",
        "\n",
        "#     return insert_newlines(answer_text)"
      ],
      "metadata": {
        "id": "q7U8a4DUQ-VA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title chat completion ex\n",
        "# async def async_get_answer(self, system:str = default_system, query:str = None):\n",
        "#       '''Асинхронная функция получения ответа от chatgpt\n",
        "#       '''\n",
        "#       # релевантные отрезки из базы\n",
        "#       docs = self.db.similarity_search(query, k=4)\n",
        "#       message_content = '\\n'.join([f'{doc.page_content}' for doc in docs])\n",
        "#       messages = [\n",
        "#             {\"role\": \"system\", \"content\": system},\n",
        "#             {\"role\": \"user\", \"content\": f\"Ответь на вопрос клиента. Не упоминай документ с информацией для ответа клиенту в ответе. Документ с информацией для ответа клиенту: {message_content}\\n\\nВопрос клиента: \\n{query}\"}\n",
        "#       ]\n",
        "\n",
        "#       # получение ответа от chatgpt\n",
        "#       completion = await openai.ChatCompletion.acreate(model=\"gpt-3.5-turbo\",\n",
        "#                                                   messages=messages,\n",
        "#                                                   temperature=0)\n",
        "\n",
        "#       return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "OniPV_yJZXGg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Подготовка истории диалога\n",
        "\n",
        "async def get_answer_async(text):\n",
        "    #payload = {\"text\":text}\n",
        "    payload = text\n",
        "\n",
        "    # Загружаем историю сообщений из памяти\n",
        "    history = summary_memory.load_memory_variables({})\n",
        "\n",
        "    # Проверяем, является ли история списком сообщений или строкой\n",
        "    if isinstance(history['history'], str):\n",
        "          conversation_string_from_history = history['history']\n",
        "    else:\n",
        "        # Если история - это список сообщений, то формируем строку из их содержимого\n",
        "        conversation_string_from_history = \"\\n\".join(message.content for message in history['history'])\n",
        "    print('conversation' + conversation_string_from_history)\n",
        "\n",
        "  # модель должна ответить на вопрос в соответствии с промптом:\n",
        "    if (option == False):\n",
        "      user_prompt = user_prompt_2\n",
        "\n",
        "    output = await answer_user_quest(system_prompt, payload, user_prompt, db, 0.2, verbose, num_fragment, model, hist=conversation_string_from_history)\n",
        "\n",
        "  # Сохраняем вопрос пользователя и ответ модели в историю\n",
        "    summary_memory.save_context({\"input\": payload}, {\"output\": output})\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "6GPyosRgocwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title создаем функцию, которая будет получать ответ от модели на основании промптов, чанков и истории беседы с пользователем:\n",
        "async def answer_user_quest(system, topic, instructions, search_index, temp, verbose, k, model, hist=''):\n",
        "    client = OpenAI()\n",
        "    docs = db.similarity_search_with_score(topic, k=k)\n",
        "    message_content = '\\n '.join([f'Отрывок текста №{i+1}\\n{doc[0].page_content}' for i, doc in enumerate(docs)])\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": f\"{instructions}\\n\\nInformation from calendar:\\n{message_content}\\n\\n Chat history:\\n{hist}\\n\\n user question:{topic}\"}]\n",
        "\n",
        "    completion = client.chat.completions.create(model=model, messages=messages, temperature=temp)\n",
        "\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "5yH6_5PO05EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def text(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "\n",
        "     # выполнение запроса в chatgpt\n",
        "     first_message = await update.message.reply_text('Your request is being processed...')\n",
        "     res = await get_answer_async(update.message.text)\n",
        "     await context.bot.edit_message_text(text=res, chat_id=update.message.chat_id, message_id=first_message.message_id)\n"
      ],
      "metadata": {
        "id": "0Tc59WPvZXPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    # возвращаем текстовое сообщение пользователю\n",
        "    #await update.message.reply_text('hello, start your conversation with assistant Jane')\n",
        "    global option\n",
        "    #option with buttons\n",
        "    option = True\n",
        "    await update.message.reply_text('Select what you want to do:', reply_markup=inline_keyboard)\n",
        "\n",
        "async def clear(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    message_ids = context.chat_data.get('message_ids', [])  # Get or default to empty list\n",
        "    if message_ids:\n",
        "        await context.bot.delete_messages(\n",
        "            chat_id=update.effective_chat.id,\n",
        "            message_ids=message_ids\n",
        "        )\n",
        "    context.chat_data['message_ids'] = []  # Clear regardless of whether it was empty\n",
        "    summary_memory.clear()\n",
        "    # Send restart message\n",
        "    await update.message.reply_text('Conversation history cleared. Run /start to start again!')\n",
        "\n",
        "async def stop(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await update.message.reply_text('Stopping bot...')\n",
        "    application = context.application\n",
        "\n",
        "    # Stop the application gracefully\n",
        "    await application.stop()\n",
        "\n",
        "\n",
        "    # # Get the current event loop\n",
        "    # loop = asyncio.get_running_loop()\n",
        "\n",
        "    # # Stop the loop gracefully\n",
        "    # tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]\n",
        "    # for task in tasks:\n",
        "    #     task.cancel()\n",
        "    # await asyncio.gather(*tasks, return_exceptions=True)\n",
        "    # loop.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5yN0bBqCRkM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INLINE\n",
        "# форма inline клавиатуры\n",
        "inline_frame = [[InlineKeyboardButton(\"Schedule with Mr Erik\", callback_data=\"with_er\")],\n",
        "                [InlineKeyboardButton(\"Ask to schedule for Mr Erik\", callback_data=\"for_er\")]]\n",
        "\n",
        "# создаем inline клавиатуру\n",
        "inline_keyboard = InlineKeyboardMarkup(inline_frame)\n",
        "\n",
        "async def button(update: Update, _):\n",
        "    global option\n",
        "    # получаем callback query из update\n",
        "    query = update.callback_query\n",
        "    if query.data == \"for_er\":\n",
        "      option = False\n",
        "      await query.edit_message_text(\"My name is Jane, I'm mr Erik's assistant, he would like to setup a meeting with you, could you please share your availability the upcoming days\")\n",
        "    else:\n",
        "      await query.edit_message_text(\"To Start your conversation with assistant Jane select what you want to do\")\n",
        "    # редактируем сообщение после нажатия\n",
        "\n"
      ],
      "metadata": {
        "id": "Ru0NS5e_Om0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "def main():\n",
        "    TOKEN = userdata.get('bot_token')\n",
        "    # создаем приложение и передаем в него токен бота\n",
        "    application = Application.builder().token(TOKEN).build()\n",
        "    print('Бот запущен...')\n",
        "\n",
        "    # добавление обработчиков\n",
        "    application.add_handler(CommandHandler(\"stop\", stop, block=False))\n",
        "    application.add_handler(CommandHandler(\"clear\", clear, block=False))\n",
        "    application.add_handler(CommandHandler(\"start\", start, block=False))\n",
        "    application.add_handler(MessageHandler(filters.TEXT, text, block=False))\n",
        "    # добавляем CallbackQueryHandler (только для inline кнопок)\n",
        "    application.add_handler(CallbackQueryHandler(button))\n",
        "\n",
        "    #запуск бота\n",
        "    try:\n",
        "        application.run_polling()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Bot stopped manually.\")\n",
        "    finally:\n",
        "        print(\"Exiting bot...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FqUIEn1AZXSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "0486b7e8-6508-4f1c-ea5d-94ead72ce557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Бот запущен...\n",
            "conversation\n",
            "conversationThe human expresses happiness about an upcoming meeting with Erik, available from April 18 to 26 after 2pm. The AI requests the human's name and additional details about the meeting, such as its length and format (Zoom or offline).\n",
            "conversationThe human, named Din, expresses happiness about an upcoming meeting with Erik, available from April 18 to 26 after 2pm, and specifies it will be a 1-hour Zoom meeting. The AI requests Din to specify the exact date for the Zoom meeting within the given range.\n",
            "conversationThe human, named Din, expresses happiness about an upcoming meeting with Erik, available from April 18 to 26 after 2pm, and specifies it will be a 1-hour Zoom meeting. When the AI requests Din to specify the exact date for the Zoom meeting within the given range, Din indicates flexibility, allowing the AI to choose. The AI then presents available time slots for each date within the range for Din to select the most suitable option.\n",
            "conversationThe human, named Din, expresses happiness about an upcoming meeting with Erik, available from April 18 to 26 after 2pm, and specifies it will be a 1-hour Zoom meeting. When the AI requests Din to specify the exact date for the Zoom meeting within the given range, Din indicates flexibility, allowing the AI to choose. The AI then presents available time slots for each date within the range for Din to select the most suitable option. Din chooses option 4 from the list provided by the AI, which includes detailed time slots on various days such as April 18 at 3:00 PM and 4:00 PM, April 19 at 2:00 PM and 3:00 PM, and so forth.\n",
            "conversationThe human, named Din, expresses happiness about an upcoming meeting with Erik, available from April 18 to 26 after 2pm, and specifies it will be a 1-hour Zoom meeting. When the AI requests Din to specify the exact date for the Zoom meeting within the given range, Din indicates flexibility, allowing the AI to choose. The AI then presents available time slots for each date within the range for Din to select the most suitable option. Din chooses option 4 from the list provided by the AI, which includes detailed time slots on various days such as April 18 at 3:00 PM and 4:00 PM, April 19 at 2:00 PM and 3:00 PM, and so forth. Din confirms being okay with option 4, and the AI clarifies that this selection corresponds to April 18 at 3:00 PM for the Zoom meeting with Erik, asking Din if they should finalize this appointment.\n",
            "conversationThe human, named Din, expresses happiness about an upcoming meeting with Erik, available from April 18 to 26 after 2pm, and specifies it will be a 1-hour Zoom meeting. When the AI requests Din to specify the exact date for the Zoom meeting within the given range, Din indicates flexibility, allowing the AI to choose. The AI then presents available time slots for each date within the range for Din to select the most suitable option. Din chooses option 4 from the list provided by the AI, which includes detailed time slots on various days such as April 18 at 3:00 PM and 4:00 PM, April 19 at 2:00 PM and 3:00 PM, and so forth. Din confirms being okay with option 4, and the AI clarifies that this selection corresponds to April 18 at 3:00 PM for the Zoom meeting with Erik. When asked if they should finalize this appointment, Din confirms, and the AI proceeds to book the Zoom meeting with Mr. Erik on April 18 at 3:00 PM, reconfirming this arrangement with Din.\n",
            "Exiting bot...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot close a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-1301b608f763>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-1301b608f763>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#запуск бота\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mapplication\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot stopped manually.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36mrun_polling\u001b[0;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         return self.__run(\n\u001b[0m\u001b[1;32m    839\u001b[0m             updater_coroutine=self.updater.start_polling(\n\u001b[1;32m    840\u001b[0m                 \u001b[0mpoll_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36m__run\u001b[0;34m(self, updater_coroutine, stop_signals, close_loop)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclose_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     def create_task(\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
          ]
        }
      ]
    }
  ]
}